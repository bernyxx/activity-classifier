{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from os import listdir\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differenze tra still, walking e running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features\n",
    "labels = ['xa', 'ya', 'za', 'xg', 'yg', 'zg']\n",
    "\n",
    "still = pd.read_csv('datasets/still_19_02_2023_17_28_18.csv')\n",
    "still = still[labels]\n",
    "\n",
    "walking = pd.read_csv('datasets/walking_19_02_2023_17_10_11.csv')\n",
    "walking = walking[labels]\n",
    "\n",
    "running = pd.read_csv('datasets/running_23_02_2023_10_48_27.csv')\n",
    "running = running[labels]\n",
    "\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9), (ax10, ax11, ax12), (ax13, ax14, ax15), (ax16, ax17, ax18)) = plt.subplots(6,3)\n",
    "\n",
    "fig.set_figheight(30)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "ax1.plot(np.arange(0, still.shape[0]), still['xa'])\n",
    "ax1.set_title('Accelerometer X-axis (Still)')\n",
    "\n",
    "ax2.plot(np.arange(0, walking.shape[0]), walking['xa'])\n",
    "ax2.set_title('Accelerometer X-axis (Walking)')\n",
    "\n",
    "ax3.plot(np.arange(0, running.shape[0]), running['xa'])\n",
    "ax3.set_title('Accelerometer X-axis (Running)')\n",
    "\n",
    "\n",
    "ax4.plot(np.arange(0, still.shape[0]), still['ya'])\n",
    "ax4.set_title('Accelerometer Y-axis (Still)')\n",
    "\n",
    "ax5.plot(np.arange(0, walking.shape[0]), walking['ya'])\n",
    "ax5.set_title('Accelerometer Y-axis (Walking)')\n",
    "\n",
    "ax6.plot(np.arange(0, running.shape[0]), running['ya'])\n",
    "ax6.set_title('Accelerometer Y-axis (Running)')\n",
    "\n",
    "\n",
    "ax7.plot(np.arange(0, still.shape[0]), still['za'])\n",
    "ax7.set_title('Accelerometer Z-axis (Still)')\n",
    "\n",
    "ax8.plot(np.arange(0, walking.shape[0]), walking['za'])\n",
    "ax8.set_title('Accelerometer Z-axis (Walking)')\n",
    "\n",
    "ax9.plot(np.arange(0, running.shape[0]), running['za'])\n",
    "ax9.set_title('Accelerometer Z-axis (Running)')\n",
    "\n",
    "ax10.plot(np.arange(0, still.shape[0]), still['xg'])\n",
    "ax10.set_title('Gyroscope X-axis (Still)')\n",
    "\n",
    "ax11.plot(np.arange(0, walking.shape[0]), walking['xg'])\n",
    "ax11.set_title('Gyroscope X-axis (Walking)')\n",
    "\n",
    "ax12.plot(np.arange(0, running.shape[0]), running['xg'])\n",
    "ax12.set_title('Gyroscope X-axis (Running)')\n",
    "\n",
    "\n",
    "ax13.plot(np.arange(0, still.shape[0]), still['yg'])\n",
    "ax13.set_title('Gyroscope Y-axis (Still)')\n",
    "\n",
    "ax14.plot(np.arange(0, walking.shape[0]), walking['yg'])\n",
    "ax14.set_title('Gyroscope Y-axis (Walking)')\n",
    "\n",
    "ax15.plot(np.arange(0, running.shape[0]), running['yg'])\n",
    "ax15.set_title('Gyroscope Y-axis (Running)')\n",
    "\n",
    "ax16.plot(np.arange(0, still.shape[0]), still['zg'])\n",
    "ax16.set_title('Gyroscope Z-axis (Still)')\n",
    "\n",
    "ax17.plot(np.arange(0, walking.shape[0]), walking['zg'])\n",
    "ax17.set_title('Gyroscope Z-axis (Walking)')\n",
    "\n",
    "ax18.plot(np.arange(0, running.shape[0]), running['zg'])\n",
    "ax18.set_title('Gyroscope Z-axis (Running)')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lettura dei dataset dalla cartella datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files inside datasets folder\n",
    "files = listdir('datasets')\n",
    "\n",
    "datasets = []\n",
    "# datasets_train = []\n",
    "# datasets_test = []\n",
    "\n",
    "# for every file add a label\n",
    "def divide_and_add_activity(name, df):\n",
    "    labels = ['xa', 'ya', 'za', 'xg', 'yg', 'zg']\n",
    "    # ridivido per 1000 (i dati erano stati moltiplicati per 1000 prima di essere inviati dalla board in modo da spedire\n",
    "    # numeri interi anzichè dei numeri in virgola mobile)\n",
    "    # df[labels] = df[labels] / 1000\n",
    "\n",
    "    # aggiungo la label dell'attvità\n",
    "    if 'still' in name:\n",
    "        df['activity'] = 'still'\n",
    "    elif 'walking' in name:\n",
    "        df['activity'] = 'walking'\n",
    "    else:\n",
    "        df['activity'] = 'running'\n",
    "    return df\n",
    "\n",
    "# def split_data(df):\n",
    "#     num_rows = df.shape[0]\n",
    "#     num_test = math.floor(num_rows / 3)\n",
    "#     num_train = num_rows - num_test\n",
    "\n",
    "#     df_train = df.iloc[:num_train, :]\n",
    "#     df_test = df.iloc[num_train:, :]\n",
    "\n",
    "#     datasets_train.append(df_train)\n",
    "#     datasets_test.append(df_test)\n",
    "\n",
    "#     print(df_train.shape)\n",
    "#     print(df_test.shape)\n",
    "\n",
    "# add activity to the labels we are interested\n",
    "labels = ['xa', 'ya', 'za', 'xg', 'yg', 'zg', 'activity']\n",
    "\n",
    "for file in files:\n",
    "    df = divide_and_add_activity(file, pd.read_csv(f'datasets/{file}'))\n",
    "    df = df[labels]\n",
    "    datasets.append(df)\n",
    "    print(\"Dimensione dataset {} \".format(file), df.shape)\n",
    "\n",
    "# for df in datasets:\n",
    "#     split_data(df)\n",
    "\n",
    "# df_train = pd.concat(datasets_train, ignore_index=True)\n",
    "# df_test = pd.concat(datasets_test, ignore_index=True)\n",
    "\n",
    "df_data = pd.concat(datasets)\n",
    "\n",
    "# print(\"Dimensione merged dataset train \", df_train.shape)\n",
    "# print(\"Dimensione merged dataset test \", df_test.shape)\n",
    "print(\"Dimensione merged dataset\", df_data.shape)\n",
    "print(df_data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizzazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "num_columns = df_data.shape[1]\n",
    "\n",
    "normalizer.fit(df_data.iloc[:, 0 : num_columns-1])\n",
    "\n",
    "def normalize_df(df):\n",
    "    \n",
    "    norm_df = df.copy()\n",
    "    norm_column = normalizer.transform(df.iloc[:, 0 : num_columns-1])\n",
    "    norm_df.iloc[:, 0 : num_columns-1] = norm_column\n",
    "\n",
    "    return norm_df\n",
    "\n",
    "# norm_train_df = normalize_df(df_train)\n",
    "# norm_test_df = normalize_df(df_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def arduino_normalizer(df):\n",
    "    norm_df = df.copy()\n",
    "    labels = ['xa', 'ya', 'za', 'xg', 'yg', 'zg']\n",
    "    norm_df[labels] = norm_df[labels] / 1000.0\n",
    "    norm_df['xa'] = (norm_df['xa'] + 4.0) / 8.0\n",
    "    norm_df['ya'] = (norm_df['ya'] + 4.0) / 8.0\n",
    "    norm_df['za'] = (norm_df['za'] + 4.0) / 8.0\n",
    "    norm_df['xg'] = (norm_df['xg'] + 2000.0) / 4000.0\n",
    "    norm_df['yg'] = (norm_df['yg'] + 2000.0) / 4000.0\n",
    "    norm_df['zg'] = (norm_df['zg'] + 2000.0) / 4000.0\n",
    "    return norm_df\n",
    "\n",
    "\n",
    "norm_df = normalize_df(df_data)\n",
    "\n",
    "fig, ((ax1, ax2),( ax3, ax4), (ax5, ax6)) = plt.subplots(3,2)\n",
    "ax1.plot(np.arange(0, len(df_data.xa)), df_data.xa)\n",
    "ax3.plot(np.arange(0, len(df_data.ya)), df_data.ya)\n",
    "ax5.plot(np.arange(0, len(df_data.za)), df_data.za)\n",
    "\n",
    "ax2.plot(np.arange(0, len(norm_df.xa)), norm_df.xa)\n",
    "ax4.plot(np.arange(0, len(norm_df.ya)), norm_df.ya)\n",
    "ax6.plot(np.arange(0, len(norm_df.za)), norm_df.za)\n",
    "\n",
    "\n",
    "\n",
    "norm_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(df, time_steps, step):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "\n",
    "    num_cols = df.shape[1]\n",
    "\n",
    "    X = df.iloc[:, :num_cols - 1]\n",
    "    y = df.iloc[:, num_cols - 1:]\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        values = X.iloc[i : (i+time_steps)].values\n",
    "        labels = y.iloc[i : i + time_steps]\n",
    "        Xs.append(values)\n",
    "        ys.append(labels.mode()['activity'][0])\n",
    "    \n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# X_train , y_train = reshape_data(norm_train_df, 20, 20)\n",
    "# X_test , y_test = reshape_data(norm_test_df, 20, 20)\n",
    "\n",
    "X_data, y_data = reshape_data(norm_df, 20, 20)\n",
    "print(X_data.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder per la y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(y_train)\n",
    "\n",
    "def encode_y(y):\n",
    "    encoded_y = encoder.transform(y)\n",
    "    return encoded_y\n",
    "\n",
    "encoded_y_train = encode_y(y_train)\n",
    "encoded_y_test = encode_y(y_test)\n",
    "\n",
    "\n",
    "print(encoder.categories_)\n",
    "# print(y_train)\n",
    "print(pd.DataFrame(encoded_y_train, columns=encoder.categories_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stampa delle dimensioni\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(encoded_y_train.shape)\n",
    "print(encoded_y_test.shape)\n",
    "encoded_y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=[X_train.shape[1], X_train.shape[2]], activation='relu'))\n",
    "# model.add(LSTM(units=8, input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "# model.add(Dropout(rate=0.3))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "# model.add(Dropout(rate=0.3))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "# model.add(Dropout(rate=0.3))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(encoded_y_train.shape[1], activation = 'softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modello - ottimizzatore e funzione di perdita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping per evitare overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, encoded_y_train,\n",
    "    validation_data=(X_test, encoded_y_test),\n",
    "    # validation_split=0.1,\n",
    "    batch_size=3,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['accuracy'], 'r', label='Accuracy of training data')\n",
    "plt.plot(history.history['val_accuracy'], 'b', label='Accuracy of validation data')\n",
    "plt.plot(history.history['loss'], 'r--', label='Loss of training data')\n",
    "plt.plot(history.history['val_loss'], 'b--', label='Loss of validation data')\n",
    "plt.title('Model Accuracy and Loss')\n",
    "plt.ylabel('Accuracy and Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice per l'affidabilità della predizione sui dati di validazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=['running', 'still', 'walking'],\n",
    "                yticklabels=['running', 'still', 'walking'],\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# indice della massima probabilità\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "max_y_test = np.argmax(encoded_y_test, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, encoded_y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "piccolo test con alcuni dati in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_still = pd.read_csv('datasets/running_23_02_2023_10_48_27.csv')[labels[0:-1]]\n",
    "data_norm = normalize_df(data_still)\n",
    "\n",
    "data_input = np.array([data_norm.iloc[7:27,:], data_norm.iloc[30:50,:]])\n",
    "print(data_input.shape)\n",
    "print(data_input)\n",
    "\n",
    "res = model.predict(data_input)\n",
    "print(res)\n",
    "print(np.argmax(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_still = pd.read_csv('datasets/running_23_02_2023_10_48_27.csv')[labels[0:-1]]\n",
    "\n",
    "data_norm = normalize_df(data_still)\n",
    "\n",
    "data_input = []\n",
    "\n",
    "count_still = 0\n",
    "count_walking = 0\n",
    "count_running = 0\n",
    "\n",
    "# resize data in signals of 20 samples\n",
    "for i in range(0, data_norm.shape[0]-20, 20):\n",
    "    data_input.append(data_norm.iloc[i:(i+20), :])\n",
    "\n",
    "input = np.array(data_input)\n",
    "print(input.shape)\n",
    "\n",
    "# classify the input\n",
    "res = model.predict(input)\n",
    "print(res)\n",
    "\n",
    "# get the index of the biggest probability (0 = still, 1 = walking)\n",
    "res = np.argmax(res, axis=1)\n",
    "print(res)\n",
    "\n",
    "for value in res:\n",
    "    if value == 0:\n",
    "        count_running += 1\n",
    "    elif value == 1:\n",
    "        count_still +=1\n",
    "    else:\n",
    "        count_walking += 1\n",
    "\n",
    "print('Still: ', count_still)\n",
    "print('Walking: ', count_walking)\n",
    "print('Running: ', count_running)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('activity_classifier_model')\n",
    "\n",
    "# convert model to tensorflow lite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('activity_classifier_model')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"./activity_classifier.tflite\", \"wb\").write(tflite_model)\n",
    "\n",
    "\n",
    "import os\n",
    "basic_model_size = os.path.getsize(\"activity_classifier.tflite\")\n",
    "print(\"Model is %d bytes\" % basic_model_size)\n",
    "\n",
    "# da eseguire in un ambiente linux o colab\n",
    "# !echo \"const unsigned char model[] = {\" > /content/model.h\n",
    "# !cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
    "# !echo \"};\"                              >> /content/model.h\n",
    "\n",
    "# import os\n",
    "# model_h_size = os.path.getsize(\"model.h\")\n",
    "# print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
    "# print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2e6baf65b4912df534866df4a24f335b0ddd2a79686966292e6a89b41752c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
