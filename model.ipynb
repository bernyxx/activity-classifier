{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from os import listdir\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differenze tra still, walking e running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant features\n",
    "still = pd.read_csv('datasets/still_19_02_2023_17_28_18.csv')\n",
    "walking = pd.read_csv('datasets/walking_19_02_2023_17_10_11.csv')\n",
    "running = pd.read_csv('datasets/running_23_02_2023_10_48_27.csv')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(still.shape[1],3)\n",
    "\n",
    "fig.set_figheight(50)\n",
    "fig.set_figwidth(25)\n",
    "\n",
    "\n",
    "for i in range (still.shape[1]):\n",
    "    title = ''\n",
    "\n",
    "    if i == 0:\n",
    "        title = \"Accelerometro Asse X\"\n",
    "    elif i == 1:\n",
    "        title = \"Accelerometro Asse Y\"\n",
    "    elif i == 2:\n",
    "        title = \"Accelerometro Asse Z\"\n",
    "    elif i == 3:\n",
    "        title = \"Giroscopio Asse X\"\n",
    "    elif i == 4:\n",
    "        title = \"Giroscopio Asse Y\"\n",
    "    elif i == 5:\n",
    "        title = \"Giroscopio Asse Z\"\n",
    "    elif i == 6:\n",
    "        title = \"Magnetometro Asse X\"\n",
    "    elif i == 7:\n",
    "        title = \"Magnetometro Asse Y\"\n",
    "    elif i == 8:\n",
    "        title = \"Magnetometro Asse Z\"\n",
    "    elif i == 9:\n",
    "        title = \"Temperatura (°C)\"\n",
    "    else:\n",
    "        title = \"Umidità (%)\"\n",
    "\n",
    "    axs[i,0].plot(np.arange(0, still.shape[0]), still[still.columns[i]])\n",
    "    axs[i,0].set_title(f'{title} (Fermo)')\n",
    "\n",
    "    axs[i,1].plot(np.arange(0, walking.shape[0]), walking[walking.columns[i]])\n",
    "    axs[i,1].set_title(f'{title} (Camminata)')\n",
    "\n",
    "    axs[i,2].plot(np.arange(0, running.shape[0]), running[running.columns[i]])\n",
    "    axs[i,2].set_title(f'{title} (Corsa)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lettura dei dataset dalla cartella datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of files inside datasets folder\n",
    "files = listdir('datasets')\n",
    "\n",
    "datasets = []\n",
    "\n",
    "# for every file add a label\n",
    "def divide_and_add_activity(name, df):\n",
    "    # aggiungo la label dell'attvità\n",
    "    if 'still' in name:\n",
    "        df['activity'] = 'still'\n",
    "    elif 'walking' in name:\n",
    "        df['activity'] = 'walking'\n",
    "    else:\n",
    "        df['activity'] = 'running'\n",
    "    return df\n",
    "\n",
    "# add activity to the labels we are interested\n",
    "labels = ['xa', 'ya', 'za', 'xg', 'yg', 'zg', 'activity']\n",
    "\n",
    "for file in files:\n",
    "    df = divide_and_add_activity(file, pd.read_csv(f'datasets/{file}'))\n",
    "    df = df[labels]\n",
    "    datasets.append(df)\n",
    "    print(\"Dimensione dataset {} \".format(file), df.shape)\n",
    "\n",
    "\n",
    "df_data = pd.concat(datasets)\n",
    "\n",
    "print(\"Dimensione merged dataset\", df_data.shape)\n",
    "print(df_data)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizzazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = MinMaxScaler()\n",
    "num_columns = df_data.shape[1]\n",
    "\n",
    "normalizer.fit(df_data.iloc[:, 0 : num_columns-1])\n",
    "\n",
    "def minmax_normalizer(df):\n",
    "    \n",
    "    norm_df = df.copy()\n",
    "    norm_column = normalizer.transform(df.iloc[:, 0 : num_columns-1])\n",
    "    norm_df.iloc[:, 0 : num_columns-1] = norm_column\n",
    "\n",
    "    return norm_df\n",
    "\n",
    "\n",
    "def arduino_normalizer(df):\n",
    "    norm_df = df.copy()\n",
    "    labels = ['xa', 'ya', 'za', 'xg', 'yg', 'zg']\n",
    "    norm_df[labels] = norm_df[labels] / 1000.0\n",
    "    norm_df['xa'] = (norm_df['xa'] + 4.0) / 8.0\n",
    "    norm_df['ya'] = (norm_df['ya'] + 4.0) / 8.0\n",
    "    norm_df['za'] = (norm_df['za'] + 4.0) / 8.0\n",
    "    norm_df['xg'] = (norm_df['xg'] + 2000.0) / 4000.0\n",
    "    norm_df['yg'] = (norm_df['yg'] + 2000.0) / 4000.0\n",
    "    norm_df['zg'] = (norm_df['zg'] + 2000.0) / 4000.0\n",
    "    return norm_df\n",
    "\n",
    "\n",
    "norm_df = arduino_normalizer(df_data)\n",
    "\n",
    "# # visualizzazione dati grezzi e normalizzati dell'accelerometro\n",
    "# fig, ((ax1, ax2),( ax3, ax4), (ax5, ax6)) = plt.subplots(3,2)\n",
    "# fig.set_figheight(10)\n",
    "# fig.set_figwidth(10)\n",
    "# ax1.plot(np.arange(0, len(df_data.xa)), df_data.xa)\n",
    "# ax1.set_title('Dati grezzi')\n",
    "# ax3.plot(np.arange(0, len(df_data.ya)), df_data.ya)\n",
    "# ax5.plot(np.arange(0, len(df_data.za)), df_data.za)\n",
    "\n",
    "# ax2.plot(np.arange(0, len(norm_df.xa)), norm_df.xa)\n",
    "# ax2.set_title('Dati normalizzati')\n",
    "# ax4.plot(np.arange(0, len(norm_df.ya)), norm_df.ya)\n",
    "# ax6.plot(np.arange(0, len(norm_df.za)), norm_df.za)\n",
    "\n",
    "\n",
    "# norm_df\n",
    "\n",
    "fig, axs = plt.subplots(6,2)\n",
    "\n",
    "fig.set_figheight(25)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "\n",
    "for i in range (6):\n",
    "    title = ''\n",
    "\n",
    "    if i == 0:\n",
    "        title = \"Accelerometro Asse X\"\n",
    "    elif i == 1:\n",
    "        title = \"Accelerometro Asse Y\"\n",
    "    elif i == 2:\n",
    "        title = \"Accelerometro Asse Z\"\n",
    "    elif i == 3:\n",
    "        title = \"Giroscopio Asse X\"\n",
    "    elif i == 4:\n",
    "        title = \"Giroscopio Asse Y\"\n",
    "    elif i == 5:\n",
    "        title = \"Giroscopio Asse Z\"\n",
    "    elif i == 6:\n",
    "        title = \"Magnetometro Asse X\"\n",
    "    elif i == 7:\n",
    "        title = \"Magnetometro Asse Y\"\n",
    "    elif i == 8:\n",
    "        title = \"Magnetometro Asse Z\"\n",
    "    elif i == 9:\n",
    "        title = \"Temperatura (°C)\"\n",
    "    else:\n",
    "        title = \"Umidità (%)\"\n",
    "\n",
    "    axs[i,0].plot(np.arange(0, df_data.shape[0]), df_data[df_data.columns[i]])\n",
    "    axs[i,0].set_title(f'{title} (Fermo)')\n",
    "\n",
    "    axs[i,1].plot(np.arange(0, norm_df.shape[0]), norm_df[norm_df.columns[i]])\n",
    "    axs[i,1].set_title(f'{title} (Camminata)')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(df, time_steps, step):\n",
    "    Xs = []\n",
    "    ys = []\n",
    "\n",
    "    num_cols = df.shape[1]\n",
    "\n",
    "    X = df.iloc[:, :num_cols - 1]\n",
    "    y = df.iloc[:, num_cols - 1:]\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    for i in range(0, len(X) - time_steps, step):\n",
    "        values = X.iloc[i : (i+time_steps)].values\n",
    "        labels = y.iloc[i : i + time_steps]\n",
    "        Xs.append(values)\n",
    "        ys.append(labels.mode()['activity'][0])\n",
    "    \n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1)\n",
    "\n",
    "\n",
    "X_data, y_data = reshape_data(norm_df, 20, 20)\n",
    "print(X_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OneHotEncoder per la y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(y_train)\n",
    "\n",
    "def encode_y(y):\n",
    "    encoded_y = encoder.transform(y)\n",
    "    return encoded_y\n",
    "\n",
    "encoded_y_train = encode_y(y_train)\n",
    "encoded_y_test = encode_y(y_test)\n",
    "\n",
    "\n",
    "print(encoder.categories_)\n",
    "# print(y_train)\n",
    "print(pd.DataFrame(encoded_y_train, columns=encoder.categories_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stampa delle dimensioni\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(encoded_y_train.shape)\n",
    "print(encoded_y_test.shape)\n",
    "encoded_y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=[X_train.shape[1], X_train.shape[2]], activation='relu'))\n",
    "# model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "# model.add(Dropout(rate=0.1))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "# model.add(Dropout(rate=0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(encoded_y_train.shape[1], activation = 'softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modello - ottimizzatore e funzione di perdita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping per evitare overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, encoded_y_train,\n",
    "    validation_data=(X_test, encoded_y_test),\n",
    "    batch_size=3,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['accuracy'], 'r', label='Precisione sui dati di train')\n",
    "plt.plot(history.history['val_accuracy'], 'b', label='Precisione sui dati di validazione')\n",
    "plt.plot(history.history['loss'], 'r--', label='Perdita sui dati di train')\n",
    "plt.plot(history.history['val_loss'], 'b--', label='Perdita sui dati di validazione')\n",
    "plt.title('Precisione e Perdita del Modello')\n",
    "plt.ylabel('Precisione e Perdita')\n",
    "plt.xlabel('N. Epoca')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrice per l'affidabilità della predizione sui dati di validazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=['running', 'still', 'walking'],\n",
    "                yticklabels=['running', 'still', 'walking'],\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# indice della massima probabilità\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "max_y_test = np.argmax(encoded_y_test, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, encoded_y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "piccolo test con alcuni dati in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_still = pd.read_csv('datasets/running_23_02_2023_10_48_27.csv')[labels[0:-1]]\n",
    "data_norm = minmax_normalizer(data_still)\n",
    "\n",
    "data_input = np.array([data_norm.iloc[7:27,:], data_norm.iloc[30:50,:]])\n",
    "print(data_input.shape)\n",
    "print(data_input)\n",
    "\n",
    "res = model.predict(data_input)\n",
    "print(res)\n",
    "print(np.argmax(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_still = pd.read_csv('datasets/still_20_02_2023_15_33_07.csv')[labels[0:-1]]\n",
    "\n",
    "data_norm = minmax_normalizer(data_still)\n",
    "\n",
    "data_input = []\n",
    "\n",
    "count_still = 0\n",
    "count_walking = 0\n",
    "count_running = 0\n",
    "\n",
    "# resize data in signals of 20 samples\n",
    "for i in range(0, data_norm.shape[0]-20, 20):\n",
    "    data_input.append(data_norm.iloc[i:(i+20), :])\n",
    "\n",
    "input = np.array(data_input)\n",
    "print(input.shape)\n",
    "\n",
    "# classify the input\n",
    "res = model.predict(input)\n",
    "print(res)\n",
    "\n",
    "# get the index of the biggest probability (0 = still, 1 = walking)\n",
    "res = np.argmax(res, axis=1)\n",
    "print(res)\n",
    "\n",
    "for value in res:\n",
    "    if value == 0:\n",
    "        count_running += 1\n",
    "    elif value == 1:\n",
    "        count_still +=1\n",
    "    else:\n",
    "        count_walking += 1\n",
    "\n",
    "print('Still: ', count_still)\n",
    "print('Walking: ', count_walking)\n",
    "print('Running: ', count_running)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverti il modello keras in un modello tensorflow lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('activity_classifier_model')\n",
    "\n",
    "# convert model to tensorflow lite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('activity_classifier_model')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "with open(\"./activity_classifier.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vis = pd.read_csv('datasets/running_23_02_2023_10_48_27.csv')[labels[0:-1]]\n",
    "data_norm = minmax_normalizer(data_still)\n",
    "\n",
    "data_zero = np.zeros(20 * 6).reshape([1,20,6])\n",
    "\n",
    "res = model.predict(data_zero)\n",
    "print(res)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2e6baf65b4912df534866df4a24f335b0ddd2a79686966292e6a89b41752c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
